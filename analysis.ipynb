{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import vincenty\n",
    "import json\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Turn off chained assignment warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = 'inputs/'\n",
    "OUTPUT_PATH = 'outputs_1/'\n",
    "\n",
    "# for conveniently holding data\n",
    "class Data(object):  \n",
    "    pass\n",
    "\n",
    "results = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dictionary_to_csv(dictionary, file_name):\n",
    "    \"\"\"Convert a dictionary to a csv with a column of keys and a column of values\"\"\"\n",
    "    \n",
    "    csv = ''\n",
    "    for k, v in dictionary.items():\n",
    "        \n",
    "        csv +=  '{},{}'.format(k, v) + '\\n'\n",
    "    \n",
    "    with open(file_name, 'w') as _file:\n",
    "        _file.write(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flood_reports = pd.read_csv(INPUT_PATH + '311_2015_flooding.csv', parse_dates=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flood_reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.n_311_flooding_reports = flood_reports.shape[0]\n",
    "flood_reports.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_flood_reports(_flood_reports):\n",
    "    \"\"\"Sanity test by plotting all the reports of flooding. It looks like New York!\"\"\"\n",
    "    \n",
    "    plt.plot(_flood_reports['longitude'], _flood_reports['latitude'], '.', alpha=0.2)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.savefig(OUTPUT_PATH + 'flood_reports_by_location.png', dpi=200, bbox_inches='tight')\n",
    "\n",
    "plot_flood_reports(flood_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distortion(data):\n",
    "    \"\"\"\n",
    "    Since we will simply use the raw longitude and latitude in our clustering, we want to understand\n",
    "    how the \"distances\" found will translate to actual miles.\n",
    "    \"\"\"\n",
    "    \n",
    "    latitude_min, latitude_max = data.latitude.min(), data.latitude.max()\n",
    "    longitude_min, longitude_max = data.longitude.min(), data.longitude.max()\n",
    "    \n",
    "    # Use the vincenty distance, which calculates distance correctly for longitude and latitude\n",
    "    lat_miles = vincenty((latitude_max, longitude_min), (latitude_min, longitude_min)).miles\n",
    "    lat_degrees = latitude_max - latitude_min\n",
    "    \n",
    "    lon_miles = vincenty((latitude_max, longitude_min), (latitude_max, longitude_max)).miles\n",
    "    lon_degrees = longitude_max - longitude_min\n",
    "    \n",
    "    miles_per_degree_lat = lat_miles/lat_degrees\n",
    "    miles_per_degree_lon = lon_miles/lon_degrees\n",
    "    \n",
    "    print(\"There are {} miles per degree latitude\".format(miles_per_degree_lat))\n",
    "    print(\"There are {} miles per degree longitude\".format(miles_per_degree_lon))\n",
    "    \n",
    "    return miles_per_degree_lat, miles_per_degree_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.miles_per_degree_lat, results.miles_per_degree_lon = distortion(flood_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying storm periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of reports of flooding per day required to qualify that day as part of a storm period\n",
    "storm_period_threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many days of the year have flooding data?\n",
    "flood_reports.created_date.dt.dayofyear.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_flood_reports(data):\n",
    "    \"\"\"\n",
    "    Plot the number of flood reports per day for the year\n",
    "    \n",
    "    NB: A couple days might be missing from this plot because they had zero reports of flooding\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.plot(data.groupby(data.created_date.dt.dayofyear).size(), '.-')\n",
    "    plt.axhline(storm_period_threshold, color='black', linestyle='dashed')\n",
    "    plt.xlabel('Day of year')\n",
    "    plt.xlim(0, 365)\n",
    "    plt.ylabel('Number of flooding reports')\n",
    "    plt.savefig(OUTPUT_PATH + 'flood_reports.png', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_flood_reports(flood_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_storm_periods(data):\n",
    "    \"\"\"\n",
    "    Aggregate those consecutive days of the year with more than 50 reports of flooding. We call these\n",
    "    \"storm periods\". \n",
    "    \"\"\"\n",
    "    \n",
    "    is_storm_periods = flood_reports.groupby(data.created_date.dt.dayofyear).size() > storm_period_threshold\n",
    "    \n",
    "    storm_periods = defaultdict(list)\n",
    "    i = 0\n",
    "    is_yesterday = False\n",
    "    \n",
    "    for today, is_today in zip(is_storm_periods.index, is_storm_periods):\n",
    "        \n",
    "        # if yesterday was False, a new run is beginning\n",
    "        if not is_today and is_yesterday:\n",
    "            i += 1\n",
    "        \n",
    "        if is_today:\n",
    "            storm_periods[i].append(today)\n",
    "            \n",
    "        is_yesterday = is_today\n",
    "    \n",
    "    results.number_of_storm_periods = len(storm_periods)\n",
    "    \n",
    "    return storm_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "storm_periods = get_storm_periods(flood_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering within storm periods to identify floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_attributes(cluster):\n",
    "    \"\"\"\n",
    "    Given a cluster, a DataFrame containing the reports consituting a cluster, return some essential\n",
    "    statistics about that cluster: the earliest report, the latest report, the center of the cluster, \n",
    "    the cluster's approximate diameter, the duration from the earliest to latest report, and the number\n",
    "    of reports in the cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    latitude_max = cluster.latitude.max()\n",
    "    latitude_min = cluster.latitude.min()\n",
    "    longitude_max = cluster.longitude.max()\n",
    "    longitude_min = cluster.longitude.min()\n",
    "    \n",
    "    space_diameter = vincenty((latitude_max, longitude_min), (latitude_min, longitude_max)).miles    \n",
    "   \n",
    "    time_diameter = cluster.created_date.max() - cluster.created_date.min()\n",
    "    \n",
    "    time_start = cluster.created_date.min()\n",
    "    time_end = cluster.created_date.max()\n",
    "    \n",
    "    space_center_latitude = cluster.latitude.mean()\n",
    "    space_center_longitude = cluster.longitude.mean()    \n",
    "    \n",
    "    # this is set to a constant for the cluster by definition. We just need the value\n",
    "    storm_number = cluster.storm_number.max()\n",
    "    \n",
    "    return [time_start, time_end, space_center_latitude, space_center_longitude,\n",
    "            space_diameter, time_diameter, cluster.shape[0], storm_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_storms(_eps, _flood_reports, _storm_periods):\n",
    "    \"\"\"\n",
    "    Apply DBSCAN clustering to reports, _flood_reports, from each storm period, _storm_periods\n",
    "    using eps.\n",
    "    \"\"\"\n",
    "    \n",
    "    minimum_flood_reports = 5\n",
    "    \n",
    "    floods = []\n",
    "    # count the total number of outlier point encountered\n",
    "    n_outliers = 0\n",
    "    \n",
    "    # loop through a dictionary of an index for the storm periods keyed to a set of days belonging\n",
    "    # to that storm period\n",
    "    for storm_number, storm_days in _storm_periods.items():\n",
    "        \n",
    "        # select the flood reports from days within this storm period\n",
    "        storm_flood_reports = _flood_reports[_flood_reports.created_date.dt.dayofyear.isin(set(storm_days))]\n",
    "        \n",
    "        # take longitude and latitude and convert to matrix for sklearn\n",
    "        storm_flood_reports_matrix = storm_flood_reports[['latitude', 'longitude']].as_matrix()\n",
    "        \n",
    "        # create an instance of the DBSCAN clustering algorithm\n",
    "        dbscan = DBSCAN(eps=_eps, min_samples=minimum_flood_reports)\n",
    "        \n",
    "        # predict the cluster of each storm report \n",
    "        storm_flood_reports['cluster'] = dbscan.fit_predict(storm_flood_reports_matrix)\n",
    "        \n",
    "        # set the storm_number for all the reports\n",
    "        storm_flood_reports['storm_number'] = storm_number\n",
    "\n",
    "        # count the outlier reports, add them to the total and then remove them\n",
    "        is_outlier = (storm_flood_reports.cluster == -1)\n",
    "        n_outliers += is_outlier.sum()\n",
    "        storm_flood_reports = storm_flood_reports[~is_outlier]\n",
    "        \n",
    "        # obtain statistics about each cluster\n",
    "        storm_floods = list(storm_flood_reports.groupby('cluster').apply(cluster_attributes).values)\n",
    "        \n",
    "        floods.extend(storm_floods)\n",
    "    \n",
    "    headers = ['time_start', 'time_end', 'latitude_center', 'longitude_center',\n",
    "               'diameter', 'duration', 'number_of_reports', 'storm_number']\n",
    "    \n",
    "    floods = pd.DataFrame(floods, columns=headers)\n",
    "    \n",
    "    return floods, n_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_floods(floods):\n",
    "    \"\"\"Plot the detected flood clusters\"\"\"\n",
    "    \n",
    "    plt.plot(floods['longitude_center'], floods['latitude_center'], 'o', alpha=0.5, color='red')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.savefig(OUTPUT_PATH + 'flood_clusters_by_location.png', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eps = 0.25/results.miles_per_degree_lat\n",
    "\n",
    "floods, results.n_outliers = cluster_storms(eps, flood_reports, storm_periods)\n",
    "\n",
    "results.n_floods = len(floods)\n",
    "\n",
    "# index from 1 since this is what the rest of our analysis uses\n",
    "floods.index += 1\n",
    "floods.to_csv(OUTPUT_PATH + '311_2015_floods.csv')\n",
    "\n",
    "# store our results as a table\n",
    "dictionary_to_csv(results.__dict__, OUTPUT_PATH + 'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_floods(floods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking the clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_storms_tune_eps(_flood_reports, _storm_periods):\n",
    "    \"\"\"Vary the eps parameter to show its effect on the number of clusters and outliers. Plot results.\"\"\"\n",
    "    \n",
    "    n_clusters_by_eps = []\n",
    "    n_outliers_by_eps = []\n",
    "\n",
    "    eps_range = np.arange(0.00001, 0.050, 0.001)\n",
    "    \n",
    "    for eps in eps_range:\n",
    "    \n",
    "        _floods, n_outliers = cluster_storms(eps, _flood_reports, _storm_periods)\n",
    "        \n",
    "        n_clusters_by_eps.append(len(_floods))\n",
    "        n_outliers_by_eps.append(n_outliers)\n",
    "        \n",
    "    \n",
    "    plt.plot(eps_range, n_clusters_by_eps)\n",
    "    plt.xlabel('Neighborhood radius in miles')\n",
    "    plt.ylabel('Number of flood clusters')\n",
    "    plt.axvline(0.25/results.miles_per_degree_lat, linestyle='dashed', color='black')\n",
    "    \n",
    "    plt.twinx()\n",
    "    # spoof a line to get the label added to the legend\n",
    "    plt.plot(np.nan, label='Flood clusters')\n",
    "    plt.plot(eps_range, n_outliers_by_eps, color='green', label='Outliers')\n",
    "    plt.ylabel('Number of outlier points')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(OUTPUT_PATH + 'n_floods_n_outliers_by_esp.png', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_storms_tune_eps(flood_reports, storm_periods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
